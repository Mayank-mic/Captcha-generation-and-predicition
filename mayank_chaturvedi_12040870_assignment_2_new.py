# -*- coding: utf-8 -*-
"""Mayank  Chaturvedi 12040870 Assignment 2 new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/135tAt9V3lWFhMm4ZMacSmoLXNMDGJFUg

Name: Mayank Chaturvedi

References Used to Complete the Assignment

1. https://www.analyticsvidhya.com/blog/2021/05/i-got-your-back-cross-validation-to-models/
2.   https://machinelearningmastery.com/k-fold-cross-validation/
3. https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826

## Part A: Multi-Class Classification

Captchas were invented to prevent bots from attacking websites. But the ML models are breaking captchas. Thus, stronger captchas are being invented. In this part, we will train a ML model to break hcaptcha (https://www.hcaptcha.com/ )!!

Please download the following dataset: https://github.com/sarang-iitb/H-captcha-dataset

### 0. Load the dataset
"""

# cloning
!git clone https://github.com/sarang-iitb/H-captcha-dataset

import zipfile
def unzip_data(filename):
  """
  Unzips filename into the current working directory.

  Args:
    filename (str): a filepath to a target zip folder to be unzipped.
  """
  zip_ref = zipfile.ZipFile(filename, "r")
  zip_ref.extractall()
  zip_ref.close()

unzip_data('/content/H-captcha-dataset/hcaptcha_dataset.zip')

train_path='/content/hcaptcha_dataset/train'
test_path='/content/hcaptcha_dataset/test'

import keras.preprocessing as image
import numpy as np
import cv2
import os
import random
import matplotlib.pyplot as plt
import pickle
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

CATEGORIES=['airplane','bicycle','boat','motorbus','motorcycle','seaplane','truck']
for category in CATEGORIES:
  folder = os.path.join(train_path, category)
  print("Class :")
  print(category)
  count=0
  for img in os.listdir(folder):
    img_path=os.path.join(folder,img)
    count=count+1
  print(count)

"""### 1. Data Cleaning and Visualization

a) Check out the labels in the dataset. How many images belong to each class?

There are labels by the name
- airplane
- bicycle
- boat
- motorbus
- motorcycle
- seaplane
- truck
"""

CATEGORIES=['airplane','bicycle','boat','motorbus','motorcycle','seaplane','truck']
for category in CATEGORIES:
  folder = os.path.join(train_path, category)
  print("Class :")
  print(category)
  count=0
  for img in os.listdir(folder):
    img_path=os.path.join(folder,img)
    count=count+1
  print(count)

data=[]
CATEGORIES=['airplane','bicycle','boat','motorbus','motorcycle','seaplane','truck']
for category in CATEGORIES:
  folder = os.path.join(train_path, category)
  label=CATEGORIES.index(category)
  for img in os.listdir(folder):
    img_path=os.path.join(folder,img)
    img_arr=cv2.imread(img_path)
    img_arr=cv2.resize(img_arr,(128,128))
    data.append([img_arr,label])

"""We can see that in the Train Dataset
- Airplane has 402 images
- Bicycle has 304 images
- Boat has 528 images
- Motorbus has 256 images
- Motorcycle has 592 images
- Seaplane has 280 images
- Truck has 656 images

b) How many train and test images are present in your dataset?
"""

count=0
print("Train Set :")
for category in CATEGORIES:
  folder = os.path.join(train_path, category)
  for img in os.listdir(folder):
    count=count+1
print(count)
print("Test set :")
count=0
for category in CATEGORIES:
  folder = os.path.join(test_path, category)
  for img in os.listdir(folder):
    count=count+1
print(count)

"""c) Write a function to display a random image and its shape. Find out whether
the shape of each image is the same or not. If not then make all images of the
same shape.
"""

import matplotlib.image as mpimg
def choose_img():
  folder1=os.listdir(train_path)
  folder2=random.choice(folder1)
  new_file=os.path.join(train_path,folder2)
  d=os.listdir(new_file)
  new_img=random.choice(d)
  img=os.path.join(new_file,new_img)
  img1 = mpimg.imread(img)
  #this function imshow displays data as an image
  img2=plt.imshow(img1)
  #show the picture
  plt.show()
  print("The shape of the given image is :")
  print(img1.shape)

choose_img()

from PIL import Image
from google.colab.patches import cv2_imshow
print("Set of shapes: ")
a={}
a=set()
for category in CATEGORIES:
  folder = os.path.join(train_path, category)
  for img in os.listdir(folder):
    img_path=os.path.join(folder,img)
    img1 = mpimg.imread(img_path)
    # img1=.resize(img1,[128,128])
    a.add(img1.shape)
for i in a:
  print(i)



b={}
b=set()
for category in CATEGORIES:
  folder = os.path.join(train_path, category)
  for img in os.listdir(folder):
    img_path=os.path.join(folder,img)
    img_n=Image.open(img_path)
    img_new=img_n.resize((128,128))
    img_new.save(img_path)
    b.add(img1.shape)

for i in b:
  print(i)

"""We can see that there are images with different dimensions

All the images have been resized to 128*128
"""

#checking the dimensions of all the images
print("Set of shapes: ")
a={}
a=set()
for category in CATEGORIES:
  folder = os.path.join(train_path, category)
  for img in os.listdir(folder):
    img_path=os.path.join(folder,img)
    img1 = mpimg.imread(img_path)
    # img1=.resize(img1,[128,128])
    a.add(img1.shape)
for i in a:
  print(i)

"""d) Do you think removing color channels (R, G, B) from images would lead to
poor modeling? How can you justify converting each image to greyscale? What
will be the effect of using a colored 3-channel image over a grayscale one on the
classification modelâ€™s performance?

Ans. No I don't think so removing color channels (R,G,B) from images would lead to poor modelling. Having more layers would lead to an unnecessary increase in weight of uneanted features which would lead to overfitting and decrease in accuracy on our
testing set. And colour over here is not a differentiating factor. In other big images it might be but here it's not the most significant feature to distinguish.

---

**Justifying converting each image to grayscale**
**Dimension Reduction :** Since an R,G,B contains three dimensions it would be difficult to interpret while greyscale has only one dimension therefore it would be easy to train our models.

---

Effect of using a colored 3-channel imageover a grayscale one:
- For RGB scale images, the image contains each component i.e R, G, B, different intensity labels. An RGB image is represented by 3 channels. Each channel typically consists of 8 bits. So for  color images there is an intensity for each scale. Therefore, large amounts of data must be stored and/or processed. Leading to a difficult task.
- **Algorithms :** Since many algorithms are built to execute on grayscale images therefore it would be convenient to use graysacle images.

e) Should you normalize your color channel values? Based on your answers do
the steps you think will be best for your model.

Yes we need to normalize our dataset.
"""

from PIL import Image
import numpy as np
import sys
import os
import csv
new_label=[]
new_label.append("Labels")
#Useful function
def createFileList():
  fileList = []
  categories=['airplane','motorbus','bicycle','seaplane','truck','motorcycle','boat']
  for files in categories:
      folders=os.path.join(train_path, files)
      for img in os.listdir(folders) :
          fullName = os.path.join(folders, img)
          fileList.append(fullName)
          new_label.append(files)
  return fileList
          # print(fullName)
  # load the original image
myFileList=createFileList()
with open("img_pixels.csv", 'w')as f:
    writer = csv.writer(f)
    writer.writerow(i for i in range(16384))
# print(new_label)
print(len(new_label))

i=0
for file in myFileList:
      # print(file)
      img_file = Image.open(file)
      # img_file.show()

      # get original image parameters...
      width, height = img_file.size
      format = img_file.format
      mode = img_file.mode

      # Make image Greyscale
      img_grey = img_file.convert('L')
      #img_grey.save('result.png')
      #img_grey.show()

      # Save Greyscale values
      value = np.asarray(img_grey.getdata(), dtype=np.int).reshape((img_grey.size[1], img_grey.size[0]))
      value = value.flatten()
      i=i+1
      # print(value)
      with open("img_pixels.csv", 'a') as f:
          writer = csv.writer(f)
          writer.writerow(value)

"""Above we have converted our image dataset from a 3-D to 1-D array so that we can easily perform our algorithms on them.

Adding the label column to our image dataset
"""

from csv import writer
from csv import reader
def add_column_in_csv(input_file, output_file, transform_row):
    """ Append a column in existing csv using csv.reader / csv.writer classes"""
    # Open the input_file in read mode and output_file in write mode
    with open(input_file, 'r') as read_obj, \
            open(output_file, 'w', newline='') as write_obj:
        # Create a csv.reader object from the input file object
        csv_reader = reader(read_obj)
        # Create a csv.writer object from the output file object
        csv_writer = writer(write_obj)
        # Read each row of the input csv file as list
        for row in csv_reader:
            # Pass the list / row in the transform function to add column text for this row
            transform_row(row, csv_reader.line_num)
            # Write the updated row / list to the output file
            csv_writer.writerow(row)


add_column_in_csv('img_pixels.csv', 'img_pixels_new.csv', lambda row, line_num: row.append(new_label[line_num - 1]))

import pandas as pd
train_data=pd.read_csv("/content/img_pixels_new.csv")

train_data

"""We can see our whole dataset has been labelled

Next we will perform normalization on our dataset by dividing the whole dataset by 255. So that all our data values are in the range of [0,1]
"""

train_label=train_data["Labels"].copy()
data_train=train_data.drop(['Labels'],axis=1).copy()
data_train=data_train/255
train_data=pd.concat([data_train,train_label],axis=1)
train_data

"""f) Visualize 3 random training images along with the labels for each class. The
dataset was manually labeled, do you spot any errors in the labels?
"""

for i in range(3):
  folder1=os.listdir(train_path)
  folder2=random.choice(folder1)

  print("Label :",folder2)
  print()
  new_file=os.path.join(train_path,folder2)
  d=os.listdir(new_file)
  new_img=random.choice(d)
  img=os.path.join(new_file,new_img)
  img1 = mpimg.imread(img)
  #this function imshow displays data as an image
  img2=plt.imshow(img1)
  #show the picture
  plt.show()
  print()

"""Yes there are some errors in putting images into their correct labels. Airplane is having some images of seaplanes, cycle is having some images of motorcycle and motorbus is having some images of trucks and vice-versa.

### 2. Preparing Balanced Samples for Training using only the Training set provided to you.

a) Do you think if you apply cross-validation to the dataset then all the crossvalidation folds will be similar? Why or why not?

No appplying cross-validation won't assure us that all the crossvalidation folds will be different. It might happen that our model gets trained on images of same class and and when it's time for testing it produces wrong results because it was trained on images from same class.

b) What can you do to ensure that every fold contains images from each class and no duplicates? Do the needful on your dataset that will ensure similar crossvalidation folds.
"""

train_data =train_data.sample(frac=1).reset_index(drop=True)
train_data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test =train_test_split(data_train,train_label, test_size=0.2, random_state=1, stratify=train_label)
y_test.value_counts()

"""we can see that our data has been splitted in appropriate proportion. The class with the large samples has got more images in test set maintaining the proportion.

### 3. Is Logistic regression a good algorithm for your dataset? Why or why not Based on the classification algorithms taught in class, which algorithms can be used to solve this problem?
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
logit =LogisticRegression()
logit.fit(X_train, y_train)

print('Accuracy : ',accuracy_score(y_test, logit.predict(X_test)) )

"""The reason is that the target label has no linear correlation with the features. In such cases, logistic regression (or linear regression for regression problems) canâ€™t predict targets with good accuracy (even on the training data).
Since our dataset features don't have a linear coorelation with the labels therfore logistic won't be a good model for us. Also the time complexity for logistic will be too high for modelling.

Algorithms such as KNN classifier and SVM can be used to train our models.

### 4. Train the KNN classifier algorithm on the training dataset. Donâ€™t use the test set at this time.

a) What distance/similarity function should we use for comparing images?
Compare at least 3 different metrics on a few random samples to get some
understanding of how they work.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier(n_neighbors=5,weights='distance', algorithm='auto', metric='cosine',metric_params=None)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
#print(X_train)
# accuracy_score(y_test, y_pred)
#print(y_pred)
print(knn.score(X_test, y_test))

knn = KNeighborsClassifier(n_neighbors=5,weights='distance', algorithm='auto', metric='euclidean',metric_params=None)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
# accuracy_score(y_test, y_pred)
print(knn.score(X_test, y_test))

knn = KNeighborsClassifier(n_neighbors=5,weights='distance', algorithm='auto', metric='cityblock',metric_params=None)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
# accuracy_score(y_test, y_pred)
print(knn.score(X_test, y_test))

knn = KNeighborsClassifier(n_neighbors=5,weights='distance', algorithm='auto', metric='hamming',metric_params=None)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
# accuracy_score(y_test, y_pred)
print(knn.score(X_test, y_test))

"""We can use the euclidean , cosine as well as cityblocks as the distance function to train our model.

b) What is the impact of increasing K on the speed and accuracy of the
algorithm?
"""

# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.model_selection import train_test_split
# from sklearn.datasets import load_iris
# from sklearn.metrics import accuracy_score
# for i in range(5):
#   print("Value of k: ", 5+i)
#   knn = KNeighborsClassifier(n_neighbors=5+i,weights='distance', algorithm='auto', metric='cosine',metric_params=None)
#   knn.fit(X_train,y_train)
#   y_pred = knn.predict(X_test)
#   #print(X_train)
#   # accuracy_score(y_test, y_pred)
#   #print(y_pred)
#   print(knn.score(X_test, y_test))

#   knn = KNeighborsClassifier(n_neighbors=5+i,weights='distance', algorithm='auto', metric='euclidean',metric_params=None)
#   knn.fit(X_train,y_train)
#   y_pred = knn.predict(X_test)
#   # accuracy_score(y_test, y_pred)
#   print(knn.score(X_test, y_test))

#   knn = KNeighborsClassifier(n_neighbors=5+i,weights='distance', algorithm='auto', metric='cityblock',metric_params=None)
#   knn.fit(X_train,y_train)
#   y_pred = knn.predict(X_test)
#   # accuracy_score(y_test, y_pred)
#   print(knn.score(X_test, y_test))

#   knn = KNeighborsClassifier(n_neighbors=5+i,weights='distance', algorithm='auto', metric='hamming',metric_params=None)
#   knn.fit(X_train,y_train)
#   y_pred = knn.predict(X_test)
#   # accuracy_score(y_test, y_pred)
#   print(knn.score(X_test, y_test))

"""c) Choose a good value of K and the distance metric based on cross-validation."""



"""### 5. Train SVM classifiers on the dataset. Donâ€™t use the test set at this time.

a) Which kernel functions can be used for this dataset? Why?

Polynomial Kernel and RBF Kernel can be used for this dataset classification because our dataset has multi-class therefore it can't be determined by linear separation .

b) Which of the SVM implementations (SVC, SGDClassifier, NuSVC and LinearSVC)
is the best for this dataset? Why?
"""

from sklearn.svm import SVC
classifierx = SVC(kernel ='rbf')
 # training set in x, y axis
classifierx.fit(X_train, y_train).predict(X_test)
classifierx.score(X_test, y_test)

K2 =SVC(kernel='linear')
K2.fit(X_train, y_train).predict(X_test)
K2.score(X_test, y_test)

K4 =SVC(kernel = 'poly', degree=2)
K4.fit(X_train, y_train)
K4.score(X_test, y_test)

from sklearn.svm import NuSVC
K5=NuSVC(kernel = 'rbf')
K5.fit(X_train, y_train)
K5.score(X_test, y_test)

K7 = NuSVC(kernel='poly')
K7.fit(X_train, y_train)
K7.score(X_test, y_test)

"""c) Finalize your SVM model based on cross-validation

"""

# from sklearn.model_selection import cross_val_score
# print(cross_val_score(K4, X_train, y_train, cv=20, scoring='accuracy').mean())
# print(cross_val_score(K5, X_train, y_train, cv=20, scoring='accuracy').mean())
# print(cross_val_score(K7, X_train, y_train, cv=20, scoring='accuracy').mean())
# print(cross_val_score(K2, X_train, y_train, cv=20, scoring='accuracy').mean())
# print(cross_val_score(classifier, X_train, y_train, cv=10, scoring='accuracy').mean())

"""### 6. Evaluate and compare the classification models on the test set provided to you

a) Calculate the classification accuracy of both the models and compare
"""

from sklearn.model_selection import cross_val_score
print(cross_val_score(classifierx, X_train, y_train, cv=10, scoring='accuracy').mean())
print(cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy').mean())

"""b) Which are the two classes that were most confused by your model? What
classification evaluation metric can be used to best visualize it? Plot the
visualization for both models
"""

from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
classifier = SVC(kernel ='rbf').fit(X_train, y_train)
 # training set in x, y axis
#classifier.fit(X_train, y_train)
#classifier.score(X_test, y_test)
y_pred1=classifier.predict(X_test)


confusion = confusion_matrix(y_test, y_pred1)
print('Confusion Matrix\n')
print(confusion)

import seaborn as sns
categories=['airplane','bicycle','boat','motorbus','motorcycle','seaplane','truck']
plt.subplots(figsize=(10, 7))
sns.heatmap(confusion, annot=True, cmap="hot", fmt='g', xticklabels=categories, yticklabels=categories)
plt.show()

# categories=['airplane','bicycle','boat','motorbus','motorcycle','seaplane','truck']
y_pred2=knn.predict(X_test)
confusion2 =confusion_matrix(y_test, y_pred2)
print('Confusion Matrix\n')
print(confusion2)

plt.subplots(figsize=(10, 7))
sns.heatmap(confusion2, annot=True, cmap="hot", fmt='g',xticklabels=categories, yticklabels=categories)
plt.show()

"""Motorbus and Truck were the most confused classes."""

categories=['airplane','bicycle','boat','motorbus','motorcycle','seaplane','truck']
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred1, target_names=categories))

print(classification_report(y_test, y_pred2, target_names=categories))

"""c) Do you think you can maximize both Precision and Recall for a model? What
do you think will be a better metric to judge a model?

We can't maximize both Precision and Recall for a model at the same time if we try increasing Precision then Recall decreases and vice-versa. Better metric to judge a model is F1-Score. where we can the idea of both precision as well as recall.

d) A bridge in Raipur is unstable and we want to avoid fatalities. Trucks and
Buses should be stopped while motorbikes and bicycles can be allowed. Which
of your 2 models do you think will be better for this task? Explain with the help of
Precision/Recall, ROC curve and AUC.

K Neighbours Classifiers model can be used for this as we want to classify between motorbus/truck vs others which can be done by KNN.
"""

# from sklearn.metrics import PrecisionRecallDisplay

# display = PrecisionRecallDisplay.from_estimator(
#     classifier, X_test, y_test, name="SVM"
# )
# _ = display.ax_.set_title("2-class Precision-Recall curve")

"""e) Find the Micro and Macro F1 Score of both of your models and comment on
which is better and why.

Macro average gives better estimate of the model's overall performance.
From b part we can see:
Model KNN Micro F1:0.72
          Macro F1:0.71
Model SVM Micro F1:0.85
          Macro F1:0.84
"""



"""### 7. Write a function to create a random 3x3 captcha matrix and ask the user to label the class with maximum number of examples. Estimate how often your best model will be able to solve the captcha problem correctly.

"""

my_result=""
def create_my_captcha():
  new_list=[]
  list_captcha_images_path=[]
  for i in range(7):
    folder1=os.listdir(train_path)
    folder2=random.choice(folder1)

    # new_file=os.path.join(train_path,folder2)
    # d=os.listdir(new_file)
    if(i==0):
      for j in range(3):
        new_file=""
        new_file=os.path.join(train_path,folder2)
        d=os.listdir(new_file)
        new_img=random.choice(d)

        img=os.path.join(new_file,new_img)
        #print(img)
        list_captcha_images_path.append(img)
        img1 = mpimg.imread(img)
        new_list.append(img1)
    #this function imshow displays data as an image
        # img2=plt.imshow(img1)
    #show the picture
        # plt.show()
    else:
      new_file=os.path.join(train_path,folder2)
      d=os.listdir(new_file)
      new_img=random.choice(d)
      img=os.path.join(new_file,new_img)
      img1 = mpimg.imread(img)
      list_captcha_images_path.append(img)
    #this function imshow displays data as an image
      # img2=plt.imshow(img1)
    #show the picture
      new_list.append(img1)
      # plt.show()
      random.shuffle(new_list)
  # k=0
  # f, axarr = plt.subplots(3,3,figsize=(6,5))

  # for i in range(3):
  #   for j in range(3):
  #     axarr[i,j].axis('off')
  #     axarr[i,j].imshow(new_list[k],interpolation='nearest', aspect='equal')
  #     f.tight_layout()
  #     k+=1

  # f.subplots_adjust(hspace=0.0,wspace=0.0, right=0.8)



  with open("img_pixels_captcha_.csv", 'w')as f:
      writer = csv.writer(f)
      writer.writerow(i for i in range(16384))
  # print(new_label)



  for file in list_captcha_images_path:
        # print(file)
        img_file = Image.open(file)
        # img_file.show()

        # get original image parameters...
        width, height = img_file.size
        format = img_file.format
        mode = img_file.mode

        # Make image Greyscale
        img_grey = img_file.convert('L')
        #img_grey.save('result.png')
        #img_grey.show()

        # Save Greyscale values
        value = np.asarray(img_grey.getdata(), dtype=np.int).reshape((img_grey.size[1], img_grey.size[0]))
        value = value.flatten()
        # print(value)
        with open("img_pixels_captcha_.csv", 'a') as f:
            writer = csv.writer(f)
            writer.writerow(value)

  import pandas as pd
  train_data_captcha=pd.read_csv("/content/img_pixels_captcha_.csv")
  # train_data_captcha

  train_data_captcha =train_data_captcha.sample(frac=1).reset_index(drop=True)
  train_data_captcha=train_data_captcha/255

  # knn = KNeighborsClassifier(n_neighbors=5,weights='distance', algorithm='auto', metric='euclidean',metric_params=None)
  # knn.fit(X_train,y_train)
  y_pred = classifier.predict(train_data_captcha)
  # accuracy_score(y_test, y_pred)
  #print(y_pred)
  #print(type(y_pred))
  lis = y_pred.tolist()
  max = 0
  res = lis[0]
  for i in lis:
      freq = lis.count(i)
      if freq > max:
          max = freq
          res = i
  my_result=res
  k=0
  f, axarr = plt.subplots(3,3,figsize=(6,5))

  for i in range(3):
    for j in range(3):
      axarr[i,j].axis('off')
      axarr[i,j].imshow(new_list[k],interpolation='nearest', aspect='equal')
      f.tight_layout()
      k+=1

  f.subplots_adjust(hspace=0.05,wspace=0.0, right=0.82)
  return res

my_result = create_my_captcha()

input("Which of the following images have highest frequency/ maximum count :")

print(my_result)

"""## Part B: Ensemble Models

You started a Car Selling business and you are giving recommendations to people for buying cars.

We have provided a dataset to help you start your business.


Dataset: https://github.com/sarang-iitb/Car_Condition_evaluation_dataset


Now, your job is to train a robust model and we will test how good you are on the test set.
"""

# get the dataset
!wget 'https://github.com/sarang-iitb/Car_Condition_evaluation_dataset'

# get the dataset
!wget 'https://raw.githubusercontent.com/sarang-iitb/Car_Condition_evaluation_dataset/main/training_data.csv'
!wget 'https://raw.githubusercontent.com/sarang-iitb/Car_Condition_evaluation_dataset/main/testing_data.csv'

"""### Data Preparation: Perform necessary transformations on the input dataset to prepare it for ML model training."""

# importing the Training and Testing dataset

import pandas as pd
Train_data = pd.read_csv('/content/training_data.csv')
Test_data = pd.read_csv('/content/testing_data.csv')
Test_data = Test_data.drop(['Deal_num'], axis=1)

#checking out the Training dataset
Train_data

Test_data

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#checking out column features
Train_data.head()

#looking at the values contained by the features
Train_data.describe()

Train_data.shape

#looking out for any null values
Train_data.isna().sum()

Train_data.columns

Train_data.dtypes

"""Since we have got the dataset values in the form of object so we need to perform encoding which can be done manually or by label encoding"""

#Function for manually encoding
Buying_Cost = {ni: n for n, ni in enumerate(set(Train_data['Buying_Cost']))}
Maintainance_Cost = {ni: n for n, ni in enumerate(set(Train_data['Maintainance_Cost']))}
Number_of_doors = {ni: n for n, ni in enumerate(set(Train_data['Number_of_doors']))}
Number_of_Passenger = {ni: n for n, ni in enumerate(set(Train_data['Number_of_Passenger']))}
Luggage_Space = {ni: n for n, ni in enumerate(set(Train_data['Luggage_Space']))}
Safety_Features = {ni: n for n, ni in enumerate(set(Train_data['Safety_Features']))}
How_is_the_deal = {ni: n for n, ni in enumerate(set(Train_data['How_is_the_deal']))}

Buying_Cost

Train_data['Buying_Cost'].unique()

How_is_the_deal

print("Buying_Cost = {}".format(Buying_Cost))
print("Maintainance_Cost = {}".format(Maintainance_Cost))
print("Number_of_doors = {}".format(Number_of_doors))
print("Number_of_passenger = {}".format(Number_of_Passenger))
print("Luggage_Space = {}".format(Luggage_Space))
print("Safety_Features = {}".format(Safety_Features))
print("How_is_the_deal = {}".format(How_is_the_deal))

Train_data.head()

#using the labelencoder to change the object values to numerical values
from sklearn.preprocessing import LabelEncoder
lb=LabelEncoder()

for i in Train_data.columns:
  Train_data[i]=lb.fit_transform(Train_data[i])

Train_data.head()
#now we can have a look at the tabel

Train_data.dtypes

"""We can see that most of our data has been encoded and now we are to perform our
training models.
"""

plt.figure(figsize=(10,6))
sns.heatmap(Train_data.corr(), annot=True)

Train_data.columns

Train_data.describe()

"""We can see some significant insights of the dataset we are having."""

#Shuffling our dataset
Train_data =Train_data.sample(frac=1).reset_index(drop=True)
Train_data

#Dividing the feature and the how. is the deal column
Xfeatures = Train_data[['Buying_Cost', 'Maintainance_Cost', 'Number_of_doors', 'Number_of_Passenger','Luggage_Space', 'Safety_Features']]
YLabels =Train_data['How_is_the_deal']

"""#Splitting the Dataset"""

X_train2, X_test2,Y_train2, Y_test2 =train_test_split(Xfeatures, YLabels, test_size=0.20, random_state=7)

# Using - Logistic Regression
logit =LogisticRegression()
logit.fit(X_train2, Y_train2)

print("Accuracy_Score: ", accuracy_score(Y_test2, logit.predict(X_test2)))

"""### Ensemble Model: Train any ensemble model of your choice combining your favorite models. In particular, we would like you to try both bagging and boosting.
* Bag of models of your choice
* Adaboost or XGBoost

"""

from sklearn.utils import check_random_state
from sklearn.ensemble import BaggingClassifier
from sklearn import tree

from sklearn.metrics import confusion_matrix
from sklearn import metrics
bagging =BaggingClassifier(tree.DecisionTreeClassifier(), max_samples =0.5, max_features =0.5)
bagging.fit(X_train2, Y_train2)
Y_pred_Bag=bagging.predict(X_test2)

print('Accuracy: ', np.round(metrics.accuracy_score(Y_test2, Y_pred_Bag), 4))
print('Precision: ', np.round(metrics.precision_score(Y_test2, Y_pred_Bag, average ='weighted'),4))
print('F1 Score: ', np.round(metrics.f1_score(Y_test2, Y_pred_Bag, average='weighted'),4))

print('\t\tClassification Report: \n', metrics.classification_report(Y_pred_Bag, Y_test2))

print("Confusion Matrix: \n", confusion_matrix(Y_test2, Y_pred_Bag))

from sklearn.model_selection import cross_val_score
# from sklearn.model_selection import cross
from sklearn.ensemble import AdaBoostClassifier
modelx = AdaBoostClassifier(random_state=1)
modelx.fit(X_train2, Y_train2)
modelx.score(X_test2,Y_test2)

scores1 = cross_val_score(modelx, X_train2, Y_train2, cv=10, scoring='accuracy')
print(scores1)

# for i in range(10):
#   x,x1,y,y1 =cross(X_train, Y_train, 10,i)
#   m=modelx.fit(x,x1)
#   k=m.predict(x)
#   print(metrics.f1_score(k,y1))
# k[10]

from sklearn.ensemble import GradientBoostingClassifier
model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)
model.fit(X_train2, Y_train2)
model.score(X_test2,Y_test2)

import xgboost as xgb
model2=xgb.XGBClassifier(random_state=1,learning_rate=0.01)
model2.fit(X_train2, Y_train2)
model2.score(X_test2,Y_test2)

from sklearn.preprocessing import LabelEncoder
lb2=LabelEncoder()

for i in Test_data.columns:
  Test_data[i]=lb2.fit_transform(Test_data[i])

my_test =GradientBoostingClassifier(random_state=1)
mytest = my_test.fit(Xfeatures, YLabels)
p=mytest.predict(Test_data)

p=pd.DataFrame(p)

p.to_csv('MayankChaturvedi_12040870.csv')

